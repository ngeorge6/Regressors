# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gLncr3Kkhafz38EI1oTYixY2ixiQtsw3
"""

# Commented out IPython magic to ensure Python compatibility.
# To upload our datasets from our working directory we need to mount our drive contents to the colab environment. 
# For the code to do so you can search “mount” in code snippets or use the code given below. 
# Our entire drive contents are now mounted on colab at the location “/gdrive”.

from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
pd.set_option('display.max_columns',None)#displaying long list of columns
pd.set_option('display.max_rows', None)#displaying long list of rows
pd.set_option('display.width', 1000)#width of window

from sklearn.metrics import accuracy_score #works
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
#from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC
from collections import Counter #for Smote,
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn import linear_model
# %matplotlib inline
import matplotlib.pyplot as plt  # Matlab-style plotting
import seaborn as sns
from scipy import stats
from scipy.stats import norm, skew #for some statistics
from subprocess import check_output


trainfile = r'/gdrive/My Drive/CIS 508/Group Assignment/train.csv'
trainData = pd.read_csv(trainfile)  #creates a dataframe
testfile = r'/gdrive/My Drive/CIS 508/Group Assignment/test.csv'
testData = pd.read_csv(testfile)  #creates a dataframe
print(trainData.shape)
print(testData.shape)

#Extract Target Column before doing missing value substitutions and one-hot encoding======
Target_Train_Cols = trainData["SalePrice"] #make copy of target column
trainData = trainData.drop(["SalePrice"], axis=1) #extracting training data without the target column

print(trainData.shape)

#DROP COLUMNS WITH LOTS OF MISSING VALUES===============================
#CAN ALSO DROP ROWS WITH LOTS OF MISSING VALUES
#Combine Train data and test data first so that the SAME COLUMNS are DROPPED in each
combined_Data = pd.concat([trainData, testData], keys=[0,1])

combined_Data.isnull()
print(combined_Data.isnull().shape)

#Define threshold for dropping columns
percent=int(0.6*(combined_Data.shape[0]))#drop if >40% missing data each column 
print(percent)
#Drop columns that have less than "thresh" number of non_Nans
td1=combined_Data.dropna(thresh=percent,axis=1)
print(td1.shape)

#look at what other columns have missing values
td1.isnull().sum()

#NOW IMPUTE MISSING VALUES FOR THE OTHER COLUMNS=========================
#IMPUTE (SUBSTITUTE) MEAN VALUES FOR NaN IN NUMERIC COLUMNS 
numeric=td1.select_dtypes(include=['int','float64']).columns
for num in numeric:
  td1[num]=td1[num].fillna(td1[num].mean())

#IMPUTE (SUBSTITUTE) MODE VALUES FOR NaN IN CATEGORICAL COLUMNS
train_cat_cols = td1.select_dtypes(exclude=['int','float64']).columns#selecting the categorical columns
for colss in train_cat_cols:
  '''
  if(td1.iloc[0][colss]=="N"):
        td1[colss]=td1[colss].fillna("N")
  else:
    print(td1[colss].mode())
    '''
  td1[colss]=td1[colss].fillna(td1[colss].mode()[0])
  
print(td1.head(20))

#CHECK IF THERE ARE ANY REMAINING MISSING VALUES 
td1.isnull().sum()

#DO ONE-HOT ENCODING ON CATEGORICAL VARIABLES==============================================
#The below function returns a list of categorical features which are not numeric. 
train_cat_cols = td1.select_dtypes(exclude=['float','int']).columns #selecting the categorical columns
print(train_cat_cols.shape)
print(train_cat_cols)

#If there are categorical columns which are encoded as numeric ones 
#then we need to explicitly enter the column names in a list and concatenate the two lists in python.
#ONE-HOT ENCODING-generate one-hot encoding on a common basis -THIS TAKES 30 MINS

###---combined_Data = pd.get_dummies(td1,train_cat_cols)
###---combined_Data.head(10)

#Separate Train data and test data
trainData = td1.xs(0)
testData = td1.xs(1)
print(trainData.shape)
print(testData.shape)

trainData=pd.concat([trainData,Target_Train_Cols], axis=1)
print(trainData.shape)

#use log transformation
trainData['SalePrice'] = np.log(trainData['SalePrice'])
#transformed histogram and normal probability plot
sns.distplot(trainData['SalePrice'], fit=norm);
fig = plt.figure()
res = stats.probplot(trainData['SalePrice'], plot=plt)

export_csv = trainData.to_csv(r'/gdrive/My Drive/CIS 508/Group Assignment/Preprocess_Train.csv')
exporttest_csv = testData.to_csv(r'/gdrive/My Drive/CIS 508/Group Assignment/Preprocess_Test.csv')

trainfile = r'/gdrive/My Drive/CIS 508/Group Assignment/Preprocess_Train.csv'
trainData = pd.read_csv(trainfile)  #creates a dataframe
testfile = r'/gdrive/My Drive/CIS 508/Group Assignment/Preprocess_Test.csv'
testData = pd.read_csv(testfile)  #creates a dataframe
print(trainData.shape)
print(testData.shape)

#Train data excluding target
x_train = trainData.iloc[:, :-1].copy()
x_test = testData.copy()

#Select just Target Column
y_train = trainData.iloc[:, -1].copy()

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(x_test.head())

#List of Categorical Features -------ONE HOT ENCODING
categoricalFeatures = [x for x in x_test.columns if x_test.dtypes[x] == 'object']
print(categoricalFeatures)

#Combine Train and test for one Hot Encoding
combined_Data = pd.concat([x_train,x_test], keys=[0,1])

#Do one Hot encoding for categorical features
combined_Data = pd.get_dummies(combined_Data,columns=categoricalFeatures)

#Separate Train data and test data
x_train = combined_Data.xs(0)
x_test = combined_Data.xs(1)
x_test.head()

#CONSTRUCT DEFAULT Random Forest Regressor ==================
rfr = RandomForestRegressor()
rfr.fit(x_train, y_train)
rfr_predict=rfr.predict(x_test)

pred1=pd.DataFrame(rfr_predict,columns=["SalePrice"])
pred1.head()

pd.concat([x_test['Id'],pred1],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.RandomForestRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.RandomForestRegressor.csv')
res.head()

#Hyperparameter tuning for random forest
parameters={ 'n_estimators': range(5,150,5),'min_samples_split' : range(2,100,2),'max_depth': range(1,20,2)}
rfr_random = RandomizedSearchCV(rfr,parameters,n_iter=15)
rfr_random.fit(x_train, y_train)
grid_parm_rfr=rfr_random.best_params_
print(grid_parm_rfr)

#contruct random forest using the best parameters
rfr1= RandomForestRegressor(**grid_parm_rfr)
rfr1.fit(x_train,y_train)
rfr1_predict = rfr1.predict(x_test)

pred11=pd.DataFrame(rfr1_predict,columns=["SalePrice"])
pred11.head()

pd.concat([x_test['Id'],pred11],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperRFRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperRFRegressor.csv')
res.head()

#CONSTRUCT DEFAULT Decision Tree Regressor ==================
dtr = DecisionTreeRegressor()
dtr.fit(x_train, y_train)
dtr_predict=dtr.predict(x_test)

pred2=pd.DataFrame(dtr_predict,columns=["SalePrice"])
pred2.head()

pd.concat([x_test['Id'],pred2],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.DecisionTreeRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.DecisionTreeRegressor.csv')
res.head()

#Hyperparameter tuning for decision tree
parameters={'min_samples_split' : range(2,50,2),'max_depth': range(1,20,2)}
dtr_random = RandomizedSearchCV(dtr,parameters,n_iter=15)
dtr_random.fit(x_train, y_train)
grid_parm=dtr_random.best_params_
print(grid_parm)

#Using the parameters obtained from HyperParameterTuning in the DecisionTreeClassifier 
dtr1 = DecisionTreeRegressor(**grid_parm)
dtr1.fit(x_train,y_train)
dtr1_predict = dtr1.predict(x_test)


pred21=pd.DataFrame(dtr1_predict,columns=["SalePrice"])
pred21.head()

pd.concat([x_test['Id'],pred21],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperDTRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperDTRegressor.csv')
res.head()

#CONSTRUCT DEFAULT Multi Layer Perceptron Regressor ==================
mlp = MLPRegressor()
mlp.fit(x_train, y_train)
mlp_predict=mlp.predict(x_test)

pred3=pd.DataFrame(mlp_predict,columns=["SalePrice"])
pred3.head()

pd.concat([x_test['Id'],pred3],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.MLPRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.MLPRegressor.csv')
res.head()

#Hyperparameter tuning for MLP 
parameters={'hidden_layer_sizes' : range(50,200,10)}
mlp_random = RandomizedSearchCV(mlp,parameters,n_iter=15)
mlp_random.fit(x_train, y_train)
grid_parm=mlp_random.best_params_
print(grid_parm)

#Using the parameters obtained from HyperParameterTuning in the DecisionTreeClassifier 
mlp1 = MLPRegressor(**grid_parm)
mlp1.fit(x_train,y_train)
mlp1_predict = mlp1.predict(x_test)


pred31=pd.DataFrame(mlp1_predict,columns=["SalePrice"])
pred31.head()

pd.concat([x_test['Id'],pred31],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperMLPRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperMLPRegressor.csv')
res.head()

#CONSTRUCT DEFAULT Support Vector Regressor ==================
svr = MLPRegressor()
svr.fit(x_train, y_train)
svr_predict=svr.predict(x_test)

pred4=pd.DataFrame(svr_predict,columns=["SalePrice"])
pred4.head()

pd.concat([x_test['Id'],pred4],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.SVRegressor.csv', index = None)

res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.SVRegressor.csv')
res.head()

# #Hyperparameter tuning for MLP 
# parameters={'hidden_layer_sizes' : range(50,200,10)}
# mlp_random = RandomizedSearchCV(mlp,parameters,n_iter=15)
# mlp_random.fit(x_train, y_train)
# grid_parm=mlp_random.best_params_
# print(grid_parm)

# #Using the parameters obtained from HyperParameterTuning in the DecisionTreeClassifier 
# mlp1 = MLPRegressor(**grid_parm)
# mlp1.fit(x_train,y_train)
# mlp1_predict = mlp1.predict(x_test)


# pred31=pd.DataFrame(mlp1_predict,columns=["SalePrice"])
# pred31.head()

# pd.concat([x_test['Id'],pred31],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperMLPRegressor.csv', index = None)

# res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.HyperMLPRegressor.csv')
# res.head()

# #CONSTRUCT DEFAULT Gradient Descent Regressor ==================
# from sklearn.ensemble import GradientBoostingClassifier
# sgd = SGDRegressor()
# sgd.fit(x_train, y_train)
# sgd_predict=sgd.predict(x_test)

# pred5=pd.DataFrame(sgd_predict,columns=["SalePrice"])
# pred5.head()

# pd.concat([x_test['Id'],pred5],axis=1).to_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.GradientDescentRegressor.csv', index = None)

# res=pd.read_csv('/gdrive/My Drive/CIS 508/Group Assignment/results.GradientDescentRegressor.csv')
# res.head()

from vecstack import stacking

# #STACKING MODELS =====================================================================
# print("___________________________________________________________________________________________\nEnsemble Methods Predictions using GradientBoosting, RandomForest and Decision Tree Classifier\n")

# models = [ MLPRegressor(), RandomForestClassifier(), DecisionTreeClassifier() ]
      
# S_Train, S_Test = stacking(models,                   
#                            x_train, y_train, x_test,   
#                            regression=False, 
     
#                            mode='oof_pred_bag', 
       
#                            needs_proba=False,
         
#                            save_dir=None, 
            
#                            metric=accuracy_score, 
    
#                            n_folds=4, 
                 
#                            stratified=True,
            
#                            shuffle=True,  
            
#                            random_state=0,    
         
#                            verbose=2)

# #STACKING - CONTRUCT A GRADIENT BOOSTING MODEL==============================
# model = GradientBoostingClassifier()
    
# model = model.fit(S_Train, y_train)
# y_pred = model.predict(S_Test)
# print('Final prediction score for ensemble methods: [%.8f]' % accuracy_score(y_test, y_pred))
# print("Confusion Matrix after STACKING for Boosting:")
# print(confusion_matrix(y_test,y_pred))
# print("=== Classification Report ===")
# print(classification_report(y_test,y_pred))